{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "import pickle as pkl\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import itertools\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "\n",
    "RANDOM_SEED = 7\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "INTERM_DIR = '../compiled_data'\n",
    "TRAIN_DATA_PATH = os.path.join(INTERM_DIR, 'train_data.pkl')\n",
    "MODEL_DIR = './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import build_tcn, build_lstm\n",
    "\n",
    "class PerformTraining:\n",
    "    \n",
    "    \"\"\"\n",
    "    This class performs the training of the desired model\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    seed : int\n",
    "        the integer of the seed utilised for reproducibility \n",
    "    TRAIN_DATA_PATH : str\n",
    "            a string indicating the directory containing preprocessed data split\n",
    "            into train, val and test sets\n",
    "    INTERM_DATA_DIR : str\n",
    "        a string indicating the directory containing intermediate computed data\n",
    "    MODEL_DIR : str\n",
    "        a string indicating the directory containing created models\n",
    "    model_gs_params : dict\n",
    "        a dictionary of preprocessing parameters\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    reproducible_results()\n",
    "        Sets seed and ensures all deterministic operations are reproducible\n",
    "    retrieve_data()\n",
    "        Retrieves the data given the data directory and folders\n",
    "    prepare_data(preprocessing_params, tuning=True):\n",
    "        Combines the preprocessing methods and splits the data for training \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epochs, TRAIN_DATA_PATH, INTERM_DATA_DIR, MODEL_DIR, model_gs_params, model_type, flush, save_plot):\n",
    "        \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        TRAIN_DATA_PATH : str\n",
    "            a string indicating the directory containing preprocessed data split\n",
    "            into train, val and test sets\n",
    "        INTERM_DATA_DIR : str\n",
    "            a string indicating the directory containing intermediate computed data\n",
    "        MODEL_DIR : str\n",
    "            a string indicating the directory containing created models\n",
    "        model_gs_params : list of lists\n",
    "            a list of lists containing the parameters for model training\n",
    "        \"\"\"\n",
    "\n",
    "        self.seed = 7\n",
    "\n",
    "        self.TRAIN_DATA_PATH = TRAIN_DATA_PATH\n",
    "        self.INTERM_DATA_DIR = INTERM_DATA_DIR\n",
    "        self.MODEL_DIR = MODEL_DIR\n",
    "        self.model_gs_params = model_gs_params\n",
    "        self.max_epochs = epochs\n",
    "        self.model_type = model_type\n",
    "        self.flush = flush\n",
    "        self.save_plot = save_plot\n",
    "        \n",
    "        with open(TRAIN_DATA_PATH, 'rb') as f:\n",
    "            self.X_train, self.y_train, self.X_val, self.y_val, self.X_test, self.y_test = pkl.load(f)\n",
    "            \n",
    "        # random shuffle dataset\n",
    "        p = np.random.permutation(len(self.X_train))\n",
    "        self.X_train, self.y_train = self.X_train[p], self.y_train[p]\n",
    "            \n",
    "        print(f\"The class distributions in the training set are: {np.unique(self.y_train, return_counts=True)}\")\n",
    "        print(f\"The class distributions in the validation set are: {np.unique(self.y_val, return_counts=True)}\")\n",
    "        print(f\"The class distributions in the test set are: {np.unique(self.y_test, return_counts=True)}\")\n",
    "\n",
    "        self.y_train, self.y_val, self.y_test = keras.utils.to_categorical(self.y_train), keras.utils.to_categorical(self.y_val), keras.utils.to_categorical(self.y_test)\n",
    "        \n",
    "        self.reproducible_results()\n",
    "        \n",
    "        if self.model_type == 'tcn':\n",
    "            self.perform_gs_training(build_tcn, os.path.join(self.MODEL_DIR, 'tcn'))\n",
    "            \n",
    "        if self.model_type == 'lstm':\n",
    "            self.perform_gs_training(build_lstm, os.path.join(self.MODEL_DIR, 'lstm'))\n",
    "\n",
    "    def reproducible_results(self):\n",
    "\n",
    "        \"\"\"Obtain reproducible results with keras, source: https://stackoverflow.com/a/52897216\"\"\"\n",
    "\n",
    "        # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "        os.environ['PYTHONHASHSEED'] = str(self.seed)\n",
    "\n",
    "        # 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "        random.seed(self.seed)\n",
    "\n",
    "        # 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "        tf.compat.v1.set_random_seed(self.seed)\n",
    "\n",
    "        # 5. Configure a new global `tensorflow` session\n",
    "        session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "        K.set_session(sess)\n",
    "\n",
    "    def plot_confusion_matrix(self, confusion_matrix, title, save_plot_dir):\n",
    "        \"\"\"Plots a given confusion matrix and saves it\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        confusion_matrix : ndarray\n",
    "            a numpy array of the confusion matrix\n",
    "        title : str\n",
    "            a string of the title name\n",
    "        save_plot_dir : str\n",
    "            a string of where to save the plot\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (ndarray, ndarray)\n",
    "            a tuple of the numpy arrays of the upsampled feature and label arrays\n",
    "        \"\"\"  \n",
    "        # Plot confusion matrix\n",
    "        labels = np.unique(self.y_train)\n",
    "        df_cm = pd.DataFrame(confusion_matrix, index = [i for i in np.unique(self.y_train)], columns = [i for i in np.unique(self.y_train)])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sn.heatmap(df_cm, annot=True)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title(title, ha=\"center\")\n",
    "        plt.xticks(np.arange(0.5, len(labels) + 0.5, 1), labels, rotation='horizontal')\n",
    "        plt.yticks(np.arange(0.5, len(labels) + 0.5, 1), labels, rotation='horizontal')\n",
    "        if save_plot_dir is not None: \n",
    "            plt.savefig(f'{save_plot_dir}.pdf', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def model_param_setup(self, params):\n",
    "        \"\"\"Retrieve from a given ordered list the correct parameters depending on model type\"\"\"\n",
    "        \n",
    "        if self.model_type == 'tcn':\n",
    "\n",
    "            model_params = {'n_layers' : params[0], \n",
    "                            'cnn_dropout_p' : params[1], \n",
    "                            'dense_dropout_p' : params[2], \n",
    "                            'activation' : params[3], \n",
    "                            'n_dense_layers' : params[4], \n",
    "                            'n_dense_neurons' : params[5], \n",
    "                            'batch_normalization' : params[6], \n",
    "                            'kernel_initializer' : params[7],\n",
    "                            'batch_size' : params[8],\n",
    "                            'optimizer' : params[9]}\n",
    "\n",
    "        if self.model_type == 'lstm':\n",
    "\n",
    "            model_params = {'n_layers' : params[0], \n",
    "                            'batch_size' : params[1],\n",
    "                            'lstm_neurons' : params[2], \n",
    "                            'n_dense_neurons' : params[3], \n",
    "                            'dropout' : params[4], \n",
    "                            'activation' : params[5],\n",
    "                            'kernel_initializer' : params[6],\n",
    "                            'optimizer' : params[7]}\n",
    "\n",
    "        return model_params    \n",
    "    \n",
    "    \n",
    "    def perform_gs_training(self, model_fn, checkpoint_filepath):\n",
    "\n",
    "        \"\"\"Performs cross-validated grid search training for selected model function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_fn : function\n",
    "            a function which creates a keras compiled model\n",
    "        checkpoint_filepath : str\n",
    "            a string indicating where to save the plots and grid search results \n",
    "            of the cross-validated grid search\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            a pandas dataframe containing the results of the grid search, \n",
    "            specifically average performance for given hyperparameters\n",
    "        \"\"\"  \n",
    "\n",
    "        pkl_name = os.path.join(checkpoint_filepath, 'gs_res.pkl')\n",
    "        if os.path.isfile(pkl_name) and self.flush==False:\n",
    "            with open(pkl_name, 'rb') as f:\n",
    "                gs_res = pkl.load(f)\n",
    "        else: \n",
    "            gs_res = []\n",
    "\n",
    "        for idx, params in enumerate(self.model_gs_params): \n",
    "\n",
    "            print(\"=================================================\")\n",
    "            print(\"Presenting Results for: %s/%s Hyperparameter Combination\" % (idx+1, len(self.model_gs_params)))\n",
    "\n",
    "            model_params = self.model_param_setup(params)\n",
    "            print(model_params)\n",
    "\n",
    "            batch_size = model_params['batch_size']\n",
    "\n",
    "            # Create backlog for accuracy in each fold\n",
    "            val_fold_accuracy = []\n",
    "            test_fold_accuracy = []\n",
    "\n",
    "            try:     \n",
    "\n",
    "                # Prepare the training dataset\n",
    "                train_dataset = tf.data.Dataset.from_tensor_slices((self.X_train, self.y_train))\n",
    "                train_dataset = train_dataset.shuffle(buffer_size = 1024).batch(batch_size)\n",
    "\n",
    "                # Prepare the validation dataset\n",
    "                val_dataset = tf.data.Dataset.from_tensor_slices((self.X_val, self.y_val))\n",
    "                val_dataset = val_dataset.shuffle(buffer_size = 1024).batch(batch_size)\n",
    "                \n",
    "                if self.model_type == 'lstm':\n",
    "                    # In stateful lstm, need to have full batches, i.e. a dataset size divisible by batch_size\n",
    "                    rem_last_n_train = (self.X_train.shape[0] % batch_size)\n",
    "                    if rem_last_n_train > 0:\n",
    "                        self.X_train, self.y_train = self.X_train[:-rem_last_n_train], self.y_train[:-rem_last_n_train]\n",
    "\n",
    "                    rem_last_n_val = (self.X_val.shape[0] % batch_size)\n",
    "                    if rem_last_n_val > 0:\n",
    "                        self.X_val, self.y_val = self.X_val[:-rem_last_n_val], self.y_val[:-rem_last_n_val]\n",
    "                    \n",
    "                    rem_last_n_test = (self.X_test.shape[0] % batch_size)\n",
    "                    if rem_last_n_test > 0:\n",
    "                        self.X_test, self.y_test = self.X_test[:-rem_last_n_test], self.y_test[:-rem_last_n_test]\n",
    "                    \n",
    "                model = model_fn(self.X_train, **model_params)\n",
    "\n",
    "                # Create Tensorboard\n",
    "                logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, update_freq='epoch', profile_batch=0)\n",
    "                # Model Checkpoint Callback\n",
    "                checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(checkpoint_filepath,'checkpoint'), save_weights_only=True, monitor='val_loss', mode='min', save_best_only=True)\n",
    "                # Early Stopping Callback\n",
    "                early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 100)\n",
    "\n",
    "                # Train the model\n",
    "                training_history = model.fit(self.X_train, self.y_train, batch_size=batch_size, validation_data=(self.X_val, self.y_val),\n",
    "                                             steps_per_epoch = self.X_train.shape[0] // batch_size if self.model_type == 'lstm' else None, \n",
    "                                             callbacks = [tensorboard_callback,\n",
    "                                                          early_stopping_callback,\n",
    "                                                          checkpoint_callback],\n",
    "                                             epochs=self.max_epochs, verbose=1)\n",
    "                \n",
    "\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "\n",
    "            try: \n",
    "\n",
    "                # Compute confusion matrix across validation folds, and the test set\n",
    "                def compute_confusion_matrix(set_to_predict, true_values, model):\n",
    "                    y_predicted = model.predict(set_to_predict)\n",
    "                    class_pred = np.argmax(y_predicted,axis = 1)\n",
    "                    class_true = np.argmax(true_values,axis = 1)\n",
    "                    res = metrics.confusion_matrix(class_true, class_pred)\n",
    "                    perc_acc = res / res.sum(axis=0)\n",
    "                    return perc_acc\n",
    "\n",
    "                test_accuracy = compute_confusion_matrix(self.X_test, self.y_test, model)\n",
    "                val_accuracy = compute_confusion_matrix(self.X_val, self.y_val, model)\n",
    "                \n",
    "                def plot_roc(set_to_predict, true_values, model, title_prefix, save_plot_dir):\n",
    "                    y_pred_keras = model.predict(set_to_predict)\n",
    "                    class_true = np.argmax(true_values, axis = 1)\n",
    "                    fpr_keras, tpr_keras, thresholds_keras = metrics.roc_curve(class_true, y_pred_keras[:,1])\n",
    "                    auc_keras = metrics.auc(fpr_keras, tpr_keras)\n",
    "                    \n",
    "                    plt.figure()\n",
    "                    lw = 2\n",
    "                    plt.plot(fpr_keras, tpr_keras,\n",
    "                        color=\"darkorange\", lw=lw,\n",
    "                        label=\"ROC curve (area = %0.2f)\" % auc_keras,\n",
    "                    )\n",
    "                    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "                    plt.xlim([0.0, 1.0])\n",
    "                    plt.ylim([0.0, 1.05])\n",
    "                    plt.xlabel(\"False Positive Rate\")\n",
    "                    plt.ylabel(\"True Positive Rate\")\n",
    "                    plt.title(\"%s ROC\" % title_prefix)\n",
    "                    plt.legend(loc=\"lower right\")\n",
    "                    if save_plot_dir is not None: \n",
    "                        plt.savefig(f'{save_plot_dir}.pdf', bbox_inches='tight')\n",
    "                    plt.show()\n",
    "                    \n",
    "                    return auc_keras\n",
    "                \n",
    "\n",
    "                if self.save_plot == True: \n",
    "                    string_model_params = model_params\n",
    "                    del string_model_params['optimizer']\n",
    "                    string_model_params['learning_rate'] = K.eval(model.optimizer.lr)\n",
    "\n",
    "                    string_model_params = [str(x) for x in [*string_model_params.values()]]\n",
    "                    save_plot_dir = os.path.join(checkpoint_filepath, 'plots')\n",
    "                    save_plot_val_dir = os.path.join(save_plot_dir, 'Val CM ' + ' '.join(string_model_params))\n",
    "                    save_plot_test_dir = os.path.join(save_plot_dir, 'Test CM ' + ' '.join(string_model_params))\n",
    "                else: \n",
    "                    save_plot_val_dir = None\n",
    "                    save_plot_test_dir = None\n",
    "\n",
    "                self.plot_confusion_matrix(test_accuracy, 'Test Dataset Accuracy', save_plot_test_dir)    \n",
    "                self.plot_confusion_matrix(val_accuracy, 'Validation Dataset Accuracy', save_plot_val_dir)\n",
    "                \n",
    "                test_roc = plot_roc(self.X_test, self.y_test, model, 'Test', save_plot_test_dir + 'ROC')\n",
    "                val_roc = plot_roc(self.X_val, self.y_val, model, 'Validation', save_plot_val_dir + 'ROC')\n",
    "\n",
    "                curr_gs_res = [model_params, self.model_type, test_accuracy.diagonal(), val_accuracy.diagonal(), test_roc, val_roc]\n",
    "                gs_res.append(curr_gs_res)\n",
    "                with open(pkl_name, 'wb') as f:\n",
    "                    pkl.dump(gs_res, f)\n",
    "\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "\n",
    "        return gs_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5000\n",
    "n_random_search = 40\n",
    "\n",
    "batch_size = [128]\n",
    "n_layers = [1] \n",
    "cnn_dropout_p = [None, 0.2, 0.5]\n",
    "dense_dropout_p = [None, 0.2, 0.5]\n",
    "activation = ['relu','tanh']\n",
    "n_dense_layers = [1]\n",
    "n_dense_neurons = [100]\n",
    "batch_normalization = [False, True]\n",
    "kernel_initializer = ['he_normal','glorot_uniform']\n",
    "optimizer = [Adam(learning_rate=0.001), SGD(learning_rate=0.001)] #, clipvalue=0.5\n",
    "\n",
    "model_gs_params = list(itertools.product(*[n_layers, cnn_dropout_p, dense_dropout_p, activation, n_dense_layers, \n",
    "                                           n_dense_neurons, batch_normalization, kernel_initializer, batch_size, optimizer]))\n",
    "model_randomgs_params = random.sample(model_gs_params, n_random_search) if len(model_gs_params) > n_random_search else model_gs_params\n",
    "print(\"%s Hyperparameter combinations determined\" % len(model_randomgs_params))\n",
    "\n",
    "training = PerformTraining(max_epochs, TRAIN_DATA_PATH, INTERM_DIR, MODEL_DIR,\n",
    "                           model_randomgs_params, 'tcn', flush=True, save_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_name = os.path.join(MODEL_DIR, 'tcn', 'gs_res.pkl')\n",
    "with open(pkl_name, 'rb') as f:\n",
    "    tcn_gs_res = pkl.load(f)\n",
    "    \n",
    "te_auc = [x[-1] for x in tcn_gs_res] #sum(x[-3])\n",
    "max_auc = max(te_auc)\n",
    "max_index = te_auc.index(max_auc)\n",
    "\n",
    "tcn_gs_res[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_name = os.path.join(MODEL_DIR, 'tcn', 'gs_res.pkl')\n",
    "with open(pkl_name, 'rb') as f:\n",
    "    tcn_gs_res = pkl.load(f)\n",
    "    \n",
    "te_auc = [x[-1] for x in tcn_gs_res] #sum(x[-3])\n",
    "max_auc = max(te_auc)\n",
    "max_index = te_auc.index(max_auc)\n",
    "\n",
    "tcn_gs_res[max_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 2000\n",
    "n_random_search = 20\n",
    "\n",
    "n_layers = [2]\n",
    "batch_size = [32]\n",
    "lstm_neurons = [100, 500, 1000]\n",
    "n_dense_neurons = [100, 500, 1000]\n",
    "dropout = [None, 0.2, 0.5, 0.8]\n",
    "activation = ['relu','tanh']\n",
    "kernel_initializer = ['he_normal','glorot_uniform']\n",
    "optimizer = [Adam(learning_rate=0.001), SGD(learning_rate=0.001)]\n",
    "\n",
    "model_gs_params = list(itertools.product(*[n_layers, batch_size, lstm_neurons, n_dense_neurons, dropout, activation, kernel_initializer, optimizer]))\n",
    "model_randomgs_params = random.sample(model_gs_params, n_random_search) if len(model_gs_params) > n_random_search else model_gs_params\n",
    "print(\"%s Hyperparameter combinations determined\" % len(model_randomgs_params))\n",
    "\n",
    "training = PerformTraining(max_epochs, TRAIN_DATA_PATH, INTERM_DIR, MODEL_DIR,\n",
    "                           model_randomgs_params, 'lstm', flush=True, save_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_name = os.path.join(MODEL_DIR, 'lstm', 'gs_res.pkl')\n",
    "with open(pkl_name, 'rb') as f:\n",
    "    tcn_gs_res = pkl.load(f)\n",
    "    \n",
    "te_auc = [x[-1] for x in tcn_gs_res] #sum(x[-3])\n",
    "max_auc = max(te_auc)\n",
    "max_index = te_auc.index(max_auc)\n",
    "\n",
    "tcn_gs_res[max_index]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
